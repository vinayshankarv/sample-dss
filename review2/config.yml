# Review II Scraper Configuration

# URLs to scrape
start_urls:
  - "https://example.com/rules/123"
  - "https://example.com/regulations/456"
  - "https://example.com/documents/789"

# Patterns to identify final pages (regex patterns)
final_page_patterns:
  - '/rule/\d+'
  - '/regulation/\d+'
  - '/document/\d+'
  - '/final-rule/\d+'

# Scraping behavior
concurrency_level: 3
max_retries: 3
retry_delay: 1.0
request_timeout: 30

# Output settings
output_folder: "review2/data"
output_format: "both"  # json, csv, or both
save_html: true

# Request settings
user_agents:
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
  - "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
  - "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

# Logging
log_level: "INFO"
log_file: "scraper.log"